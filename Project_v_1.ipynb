{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project v_1",
      "provenance": [],
      "authorship_tag": "ABX9TyNH94fIHq6TlUZVqJB+PNk3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SLTHRN/Heart_disease_ML/blob/main/Project_v_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9c0_WM8HPWA"
      },
      "source": [
        "### IMPORT LIBRARIES\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_talIgXHSsj"
      },
      "source": [
        "### IMPORTING CSV FILE FROM GOOGLE DRIVE\n",
        "\n",
        "import csv\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#FILE LOCATION IN GOOGLE DRIVE\n",
        "directory = '/content/drive/MyDrive/APS360PROJECT/New data.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0_jYCJmHUBE"
      },
      "source": [
        "### DEFINING CNN MODEL NETWORK\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#DEFINING A 3 LAYER FULLY CONNECTED MODEL\n",
        "class neuralNetwork(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(neuralNetwork, self).__init__()\n",
        "    self.layer1 = nn.Linear(13,50)\n",
        "    self.layer2 = nn.Linear(50,70)\n",
        "    self.layer3 = nn.Linear(70,1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #x = x.squeeze()\n",
        "    activation1 = self.layer1(x)\n",
        "    activation1 = F.relu(activation1)\n",
        "    activation2 = F.relu(self.layer2(activation1))\n",
        "    output = self.layer3(activation2)\n",
        "    return output"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO39mNObHWKc"
      },
      "source": [
        "### LOADING CSV FILE FROM DIRECTORY\n",
        "\n",
        "def load_csv(directory):\n",
        "  file = open(directory, \"r\")\n",
        "  retrieved = csv.reader(file)\n",
        "\n",
        "  data_size = -1\n",
        "  dictionary = {}\n",
        "  data = []\n",
        "\n",
        "  for row in retrieved:\n",
        "    data_size+= 1\n",
        "    if data_size == 0:\n",
        "      for i in range(len(row)):\n",
        "        dictionary[i] = row[i]\n",
        "    else:\n",
        "      data.append(row)\n",
        "\n",
        "  return data_size, data, dictionary"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiixfwB8HXpQ"
      },
      "source": [
        "### DATA STRING TO FLOAT\n",
        "\n",
        "def str_to_flt(data):\n",
        "  fail_count = 0\n",
        "\n",
        "  for i in range(len(data)):\n",
        "    row = data[i]\n",
        "    new_row = []\n",
        "    for j in range(len(row)):\n",
        "      if row[j].isnumeric():\n",
        "        new_row.append(float(row[j]))\n",
        "      else:\n",
        "        test = row[j].split('.')\n",
        "        if len(test) == 2 and (test[0].isnumeric() and test[1].isnumeric()):\n",
        "          new_row.append(float(row[j]))\n",
        "        else:\n",
        "          fail_count += 1\n",
        "          print('!!!!!! Not Numeric, count:' + str(fail_count))\n",
        "          new_row.append(row[j])\n",
        "    data[i] = new_row\n",
        "  return data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qGHyD6oHZ9J"
      },
      "source": [
        "### GET LABELS\n",
        "\n",
        "def seperate_labels(data):\n",
        "  labels = []\n",
        "\n",
        "  for row in data:\n",
        "    labels.append(int(row[-1]))\n",
        "    del row[-1]\n",
        "\n",
        "  labels = get_binary(labels)\n",
        "\n",
        "  return data, labels"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmKtKurjHbPz"
      },
      "source": [
        "### TURN LABELS INTO BINARY CLASSES\n",
        "\n",
        "def get_binary(labels):\n",
        "  new_labels = []\n",
        "  for label in labels:\n",
        "    if label == 0.0:\n",
        "      new_labels.append(0.0)\n",
        "    else:\n",
        "      new_labels.append(1.0)\n",
        "  return new_labels\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuhfNhsUHcd2"
      },
      "source": [
        "### GENERATING SAMPLERS FOR DATALOADERS\n",
        "\n",
        "def get_samplers(data_size):\n",
        "  torch.manual_seed(0)\n",
        "  index = torch.randperm(data_size)\n",
        "  end_training = int(len(index)*0.6) # Defining index for last data sample in training data\n",
        "  end_validation = end_training + int(len(index)*0.2) # Defining index for last data sample in validation set\n",
        "\n",
        "  training_sampler, validation_sampler, testing_sampler = index[:end_training], index[end_training:end_validation], index[end_validation:]\n",
        "\n",
        "  return training_sampler, validation_sampler, testing_sampler"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_GvsTo7HdmI"
      },
      "source": [
        "### GENERATING DATA LOADERS\n",
        "\n",
        "def get_loaders(data, sampler, batch_size = 64):\n",
        "  dataset = torch.tensor(data)\n",
        "\n",
        "  loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, sampler=sampler)\n",
        "  \n",
        "  return loader"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxOX1rz4HeoF"
      },
      "source": [
        "### CALCULATE ACCURACY OF MODEL\n",
        "\n",
        "def get_accuracy(model, d_loader, l_loader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for (data, label) in zip(d_loader, l_loader):\n",
        "    output = model(data)\n",
        "    for i in range(len(output)):\n",
        "      total += 1.0\n",
        "      if output[i] >= 0.5:\n",
        "        pred = 1.0\n",
        "      else:\n",
        "        pred = 0.0\n",
        "      if pred == label[i]:\n",
        "        correct += 1.0\n",
        "  return correct/total"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5xOgTHhHglf"
      },
      "source": [
        "### MODEL TRAINING\n",
        "\n",
        "def training(t_loader, t_label_loader, v_loader, v_label_loader, epochs=10, learning_rate=0.001):\n",
        "  n = 0\n",
        "  iters, losses, training_accuracy, validation_accuracy = [], [], [], []\n",
        "\n",
        "  model = neuralNetwork()\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for (data, label) in zip(t_loader, t_label_loader):\n",
        "      output = model(data)\n",
        "      loss = criterion(output.squeeze(), label)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      iters.append(n)\n",
        "      losses.append(float(loss)/batch_size)\n",
        "      training_accuracy.append(get_accuracy(model, t_loader, t_label_loader))\n",
        "      validation_accuracy.append(get_accuracy(model, v_loader, v_label_loader))\n",
        "      n += 1\n",
        "\n",
        "  plt.title(\"Training\")\n",
        "  plt.plot(iters, losses, label=\"Train\")\n",
        "  plt.xlabel(\"Iterations\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"Accuracies\")\n",
        "  plt.plot(iters, validation_accuracy, label = \"Validation\")\n",
        "  plt.plot(iters, training_accuracy, label = \"Training\")\n",
        "  plt.xlabel(\"Iterations\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.show\n",
        "\n",
        "  print(\"Final loss: {}\".format(losses[-1]))\n",
        "  print(\"Final training accuracy: {}\".format(training_accuracy[-1]))\n",
        "  print(\"Final validation accuracy: {}\".format(validation_accuracy[-1]))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMrAox8UHidh"
      },
      "source": [
        "### RUNNING THE CODE\n",
        "\n",
        "data_size, data, dictionary = load_csv(directory)\n",
        "data = str_to_flt(data)\n",
        "data, labels = seperate_labels(data)\n",
        "training_sampler, validation_sampler, testing_sampler = get_samplers(data_size)\n",
        "\n",
        "batch_size = 10\n",
        "training_loader = get_loaders(data, training_sampler, batch_size)\n",
        "training_label_loader = get_loaders(labels, training_sampler, batch_size)\n",
        "validation_loader = get_loaders(data, validation_sampler, batch_size)\n",
        "validation_label_loader = get_loaders(labels, validation_sampler, batch_size)\n",
        "testing_loader = get_loaders(data, testing_sampler, batch_size)\n",
        "testing_label_loader = get_loaders(labels, testing_sampler, batch_size)\n",
        "\n",
        "training(training_loader, training_label_loader, validation_loader, validation_label_loader)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}